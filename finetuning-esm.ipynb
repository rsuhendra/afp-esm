{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"setup\"></a>\n",
    "# üõ†Ô∏è Environment Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T16:59:14.478904Z",
     "iopub.status.busy": "2024-04-16T16:59:14.478603Z",
     "iopub.status.idle": "2024-04-16T16:59:32.928564Z",
     "shell.execute_reply": "2024-04-16T16:59:32.927805Z",
     "shell.execute_reply.started": "2024-04-16T16:59:14.478877Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    EsmForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    get_scheduler\n",
    ")\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "from datasets import Dataset, DatasetDict\n",
    "from evaluate import load\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"data\"></a>\n",
    "# üóÉÔ∏è Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T16:59:32.93046Z",
     "iopub.status.busy": "2024-04-16T16:59:32.929753Z",
     "iopub.status.idle": "2024-04-16T16:59:33.192457Z",
     "shell.execute_reply": "2024-04-16T16:59:33.191602Z",
     "shell.execute_reply.started": "2024-04-16T16:59:32.93041Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9974\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Protein_ID</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>113927</td>\n",
       "      <td>MALSLFTVGQLIFLFWTLRITEANPDPAAKAAPAAVADPAAAAAAA...</td>\n",
       "      <td>AFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>210960</td>\n",
       "      <td>MKSAILTGLLFVLLCVDHMSSASQQSVVATQLIPINTALTPIMMKG...</td>\n",
       "      <td>AFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>213510</td>\n",
       "      <td>MLAALLVCAMVALTRAANGDTGKEAVMTGSSGKNLTECPTDWKMFN...</td>\n",
       "      <td>AFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2315605</td>\n",
       "      <td>MRRQTTAIFVLLGLLAVFVVQGSTEDTGSTPTADNAPAASNGTAAP...</td>\n",
       "      <td>AFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2411496</td>\n",
       "      <td>MSFKISTFTKIWLIIAVIVMCLCNEYNCQCTGAADCTSCTAACTGC...</td>\n",
       "      <td>AFP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Protein_ID                                           Sequence Class\n",
       "0     113927  MALSLFTVGQLIFLFWTLRITEANPDPAAKAAPAAVADPAAAAAAA...   AFP\n",
       "1     210960  MKSAILTGLLFVLLCVDHMSSASQQSVVATQLIPINTALTPIMMKG...   AFP\n",
       "2     213510  MLAALLVCAMVALTRAANGDTGKEAVMTGSSGKNLTECPTDWKMFN...   AFP\n",
       "3    2315605  MRRQTTAIFVLLGLLAVFVVQGSTEDTGSTPTADNAPAASNGTAAP...   AFP\n",
       "4    2411496  MSFKISTFTKIWLIIAVIVMCLCNEYNCQCTGAADCTSCTAACTGC...   AFP"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "df = pd.read_csv(\"Dataset.csv\")\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T16:59:33.193959Z",
     "iopub.status.busy": "2024-04-16T16:59:33.193682Z",
     "iopub.status.idle": "2024-04-16T16:59:33.208329Z",
     "shell.execute_reply": "2024-04-16T16:59:33.207535Z",
     "shell.execute_reply.started": "2024-04-16T16:59:33.193935Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "NON-AFP    9493\n",
       "AFP         481\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T16:59:33.212118Z",
     "iopub.status.busy": "2024-04-16T16:59:33.211467Z",
     "iopub.status.idle": "2024-04-16T16:59:33.229361Z",
     "shell.execute_reply": "2024-04-16T16:59:33.22829Z",
     "shell.execute_reply.started": "2024-04-16T16:59:33.212092Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "0    0.951775\n",
       "1    0.048225\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new column 'labels' to contain binary labels\n",
    "df['labels'] = df['Class'].apply(lambda x: 0 if x == 'NON-AFP' else 1)\n",
    "df['labels'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced classes 95% to 5% ish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T16:59:33.231388Z",
     "iopub.status.busy": "2024-04-16T16:59:33.230596Z",
     "iopub.status.idle": "2024-04-16T16:59:33.248377Z",
     "shell.execute_reply": "2024-04-16T16:59:33.247543Z",
     "shell.execute_reply.started": "2024-04-16T16:59:33.231356Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7979\n",
      "1995\n"
     ]
    }
   ],
   "source": [
    "# train, validation split, splits unbalanced classes properly\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df['labels'],\n",
    "    random_state=42)\n",
    "\n",
    "print(len(train_df))\n",
    "print(len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T16:59:33.249739Z",
     "iopub.status.busy": "2024-04-16T16:59:33.249461Z",
     "iopub.status.idle": "2024-04-16T16:59:33.307129Z",
     "shell.execute_reply": "2024-04-16T16:59:33.306432Z",
     "shell.execute_reply.started": "2024-04-16T16:59:33.249716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# convert data into HuggingFace DatasetDict\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T16:59:33.308938Z",
     "iopub.status.busy": "2024-04-16T16:59:33.308213Z",
     "iopub.status.idle": "2024-04-16T16:59:34.872861Z",
     "shell.execute_reply": "2024-04-16T16:59:34.8719Z",
     "shell.execute_reply.started": "2024-04-16T16:59:33.308904Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7979/7979 [00:04<00:00, 1823.39 examples/s]\n",
      "/Users/richardsuhendra/miniconda3/envs/ds/lib/python3.10/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/Users/richardsuhendra/miniconda3/envs/ds/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "Map (num_proc=8): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1995/1995 [00:01<00:00, 1958.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer and tokenize data\n",
    "model_checkpoint = \"facebook/esm2_t6_8M_UR50D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize(examples, max_length=1023):\n",
    "    text = examples[\"Sequence\"]\n",
    "    encoding = tokenizer(text, truncation=True, max_length=max_length)\n",
    "    encoding[\"labels\"] = examples[\"labels\"]\n",
    "    return encoding\n",
    "\n",
    "encoded_dataset = dataset_dict.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    num_proc=os.cpu_count(),\n",
    "    remove_columns=dataset_dict[\"train\"].column_names\n",
    ")\n",
    "\n",
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"train\"></a>\n",
    "# üí™ Train Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T16:59:34.874494Z",
     "iopub.status.busy": "2024-04-16T16:59:34.874171Z",
     "iopub.status.idle": "2024-04-16T16:59:36.109982Z",
     "shell.execute_reply": "2024-04-16T16:59:36.109175Z",
     "shell.execute_reply.started": "2024-04-16T16:59:34.87446Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model checkpoint for classification\n",
    "model = EsmForSequenceClassification.from_pretrained(\n",
    "\tmodel_checkpoint,\n",
    "\tnum_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T16:59:36.111546Z",
     "iopub.status.busy": "2024-04-16T16:59:36.111245Z",
     "iopub.status.idle": "2024-04-16T16:59:36.163323Z",
     "shell.execute_reply": "2024-04-16T16:59:36.162563Z",
     "shell.execute_reply.started": "2024-04-16T16:59:36.11152Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# configure model for LoRA fine-tuning\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    inference_mode=False,\n",
    "    bias=\"none\",\n",
    "    r=8, # rank number\n",
    "    lora_alpha=16, # scaling factor)\n",
    "    lora_dropout=0.2, # dropout prob\n",
    "    target_modules=[ # which layers to apply LoRA\n",
    "        \"query\",\n",
    "        \"key\",\n",
    "        \"value\"\n",
    "    ],\n",
    "    modules_to_save=['classifier'] # ensures that the fine-tuned classifier head is saved when calling trainer.save_model later\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# adjust dropout in the classifier head\n",
    "model.base_model.model.classifier.modules_to_save.default.dropout.p = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T16:59:36.16523Z",
     "iopub.status.busy": "2024-04-16T16:59:36.164694Z",
     "iopub.status.idle": "2024-04-16T16:59:36.173288Z",
     "shell.execute_reply": "2024-04-16T16:59:36.172388Z",
     "shell.execute_reply.started": "2024-04-16T16:59:36.165201Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 195522 || all params: 8036285 || trainable%: 2.4329898703194326\n"
     ]
    }
   ],
   "source": [
    "# show amount of trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 195,522 parameters out of 8,036,285 (2.43%) are adjusted during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T16:59:36.174675Z",
     "iopub.status.busy": "2024-04-16T16:59:36.174359Z",
     "iopub.status.idle": "2024-04-16T17:02:59.648024Z",
     "shell.execute_reply": "2024-04-16T17:02:59.647003Z",
     "shell.execute_reply.started": "2024-04-16T16:59:36.17465Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardsuhendra/miniconda3/envs/ds/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/bq/d87yzfm176j8j6c_qfnfd_b00000gn/T/ipykernel_24063/960580150.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "fp16 mixed precision requires a GPU (not 'mps').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 56\u001b[0m\n\u001b[1;32m     50\u001b[0m early_stopping_callback \u001b[38;5;241m=\u001b[39m EarlyStoppingCallback(\n\u001b[1;32m     51\u001b[0m     early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     52\u001b[0m     early_stopping_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# create trainer\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoded_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoded_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping_callback\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[1;32m     67\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.10/site-packages/transformers/trainer.py:461\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_accelerator_and_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker \u001b[38;5;241m=\u001b[39m TrainerMemoryTracker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mskip_memory_metrics)\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.10/site-packages/transformers/trainer.py:5099\u001b[0m, in \u001b[0;36mTrainer.create_accelerator_and_postprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5096\u001b[0m     args\u001b[38;5;241m.\u001b[39mupdate(accelerator_config)\n\u001b[1;32m   5098\u001b[0m \u001b[38;5;66;03m# create accelerator object\u001b[39;00m\n\u001b[0;32m-> 5099\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;241m=\u001b[39m \u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5100\u001b[0m \u001b[38;5;66;03m# some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\u001b[39;00m\n\u001b[1;32m   5101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather_for_metrics\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.10/site-packages/accelerate/accelerator.py:540\u001b[0m, in \u001b[0;36mAccelerator.__init__\u001b[0;34m(self, device_placement, split_batches, mixed_precision, gradient_accumulation_steps, cpu, dataloader_config, deepspeed_plugin, fsdp_plugin, torch_tp_plugin, megatron_lm_plugin, rng_types, log_with, project_dir, project_config, gradient_accumulation_plugin, step_scheduler_with_optimizer, kwargs_handlers, dynamo_backend, dynamo_plugin, deepspeed_plugins)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnative_amp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmusa\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_xla_available(\n\u001b[1;32m    538\u001b[0m     check_is_tpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    539\u001b[0m ):\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16 mixed precision requires a GPU (not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    541\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler\u001b[38;5;241m.\u001b[39mto_kwargs() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;241m=\u001b[39m get_grad_scaler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: fp16 mixed precision requires a GPU (not 'mps')."
     ]
    }
   ],
   "source": [
    "# configure training args\n",
    "num_train_epochs = 10\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "\n",
    "args = TrainingArguments(\n",
    "    seed=42,\n",
    "    fp16=True,\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy = \"steps\",\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=4,\n",
    "    # gradient_checkpointing=True,\n",
    "    logging_steps=50,\n",
    "    eval_steps=50,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    metric_for_best_model=\"auc_roc\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='none'  # Disable Weights & Biases logging\n",
    ")\n",
    "\n",
    "# define metrics to compute during training\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    probabilities = softmax(torch.tensor(logits)).numpy()\n",
    "    predictions = np.argmax(probabilities, axis=1)\n",
    "    probabilities_pos_class = probabilities[:, 1]\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, zero_division=0)\n",
    "    recall = recall_score(labels, predictions, zero_division=0)\n",
    "    auc = roc_auc_score(labels, probabilities_pos_class)\n",
    "    auc_pr = average_precision_score(labels, probabilities_pos_class)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"auc_roc\": auc,\n",
    "        \"auc_pr\": auc_pr\n",
    "    }\n",
    "\n",
    "# define early stopping\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.01\n",
    ")\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "\tdef __init__(self, alpha=0.25, gamma=2.0):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.alpha = alpha  # Alpha for the positive class (minority class)\n",
    "\t\tself.gamma = gamma\n",
    "\n",
    "\tdef forward(self, inputs, targets):\n",
    "\t\t# Compute binary cross-entropy loss\n",
    "\t\tBCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "\t\t\n",
    "\t\t# Compute pt\n",
    "\t\tpt = torch.exp(-BCE_loss)  # pt = p if target == 1, else 1-p\n",
    "\t\t\n",
    "\t\t# Apply alpha based on the target class\n",
    "\t\talpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)  # alpha_t = alpha if target == 1, else 1-alpha\n",
    "\t\tfocal_loss = alpha_t * (1 - pt) ** self.gamma * BCE_loss\n",
    "\t\t\n",
    "\t\treturn focal_loss.mean()\n",
    "\n",
    "# Initialize Focal Loss\n",
    "loss_fn = FocalLoss(alpha=0.95, gamma=2.0)\n",
    "\n",
    "# Pass the loss function to the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback],\n",
    "    loss_fn=loss_fn\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T17:02:59.649993Z",
     "iopub.status.busy": "2024-04-16T17:02:59.649319Z",
     "iopub.status.idle": "2024-04-16T17:03:00.756265Z",
     "shell.execute_reply": "2024-04-16T17:03:00.755397Z",
     "shell.execute_reply.started": "2024-04-16T17:02:59.649962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# evaluate model on validation set\n",
    "eval_dict = trainer.evaluate()\n",
    "eval_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T17:03:00.760352Z",
     "iopub.status.busy": "2024-04-16T17:03:00.759916Z",
     "iopub.status.idle": "2024-04-16T17:03:00.986413Z",
     "shell.execute_reply": "2024-04-16T17:03:00.985305Z",
     "shell.execute_reply.started": "2024-04-16T17:03:00.760322Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# save fine-tuned LoRA adapters + classification head\n",
    "model_path = \"/content/demo_model\"\n",
    "trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"inference\"></a>\n",
    "## üéØ Inference Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T17:03:00.988065Z",
     "iopub.status.busy": "2024-04-16T17:03:00.98774Z",
     "iopub.status.idle": "2024-04-16T17:03:01.547982Z",
     "shell.execute_reply": "2024-04-16T17:03:01.546896Z",
     "shell.execute_reply.started": "2024-04-16T17:03:00.988037Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load fine-tuned model adapters onto the base model checkpoint\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "fine_tuned_model = PeftModel.from_pretrained(base_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T17:03:01.549703Z",
     "iopub.status.busy": "2024-04-16T17:03:01.549361Z",
     "iopub.status.idle": "2024-04-16T17:03:35.14827Z",
     "shell.execute_reply": "2024-04-16T17:03:35.147125Z",
     "shell.execute_reply.started": "2024-04-16T17:03:01.549676Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# generate predictions\n",
    "def predict(text, model, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        max_length=1023\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    return probabilities[:, 1].numpy()\n",
    "\n",
    "val_pred_probas = val_df['peptide'].apply(lambda x: predict(x, fine_tuned_model, tokenizer))\n",
    "\n",
    "print('ROC-AUC:', roc_auc_score(val_df['labels'], val_pred_probas))\n",
    "print('PR-AUC:', average_precision_score(val_df['labels'], val_pred_probas))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30684,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
